from langgraph.graph import MessagesState
from langchain.chat_models import init_chat_model
from pydantic import BaseModel, Field
from typing import Literal
from langgraph.graph import MessagesState
from src.pydantic_models import GradeDocuments
from src.config import ConfigManager
import logging

# Set up logging
logger = logging.getLogger(__name__)

# --- get the config ---
try:
    config = ConfigManager()._config
except Exception as e:
    logger.error(f"Failed to load config in graph_nodes.py: {e}")
    raise

# --- initialize the models ---
response_model = init_chat_model(
    config['model_config']['response_model'], 
    temperature=config['model_config']['temperature']
)

grader_model = init_chat_model(
    config['model_config']['grader_model'], 
    temperature=config['model_config']['temperature']
)


class State(MessagesState):
    documents: list[str]
    
def generate_query_or_respond(retriever_tool, state: MessagesState):
    """Call the model to generate a response based on the current state. Given
    the question, it will decide to retrieve using the retriever tool, or simply respond to the user.
    """
    # Log the messages being sent to the model
    # logger.info("=== GENERATE_QUERY_OR_RESPOND INPUT ===")
    # for msg in state["messages"]:
    #     logger.info(f"Message: {msg.content}")
    # logger.info("=== END GENERATE_QUERY_OR_RESPOND INPUT ===")
    
    response = (
        response_model
        .bind_tools([retriever_tool]).invoke(state["messages"])
    )
    
    # Log the response
    logger.info("=== GENERATE_QUERY_OR_RESPOND OUTPUT ===")
    logger.info(f"Response: {response.content}")
    if hasattr(response, 'tool_calls') and response.tool_calls:
        logger.info(f"Tool calls: {[tc.get('name') for tc in response.tool_calls]}")
    else:
        logger.info("No tool calls - responding directly")
    logger.info("=== END GENERATE_QUERY_OR_RESPOND OUTPUT ===")
    
    return {"messages": [response]}


def grade_documents(
    state: MessagesState,
) -> Literal["generate_answer", "rewrite_question"]:
    """Determine whether the retrieved documents are relevant to the question."""
    question = state["messages"][0].content
    context = state["messages"][-1].content

    # Count how many times we've been through this loop
    # Look for actual rewrite_question node executions in the message history
    rewrite_count = 0
    for msg in state["messages"]:
        if hasattr(msg, 'content') and msg.content:
            # Check if this message was generated by the rewrite_question node
            # We can identify this by looking for patterns that indicate a rewritten question
            if any(keyword in msg.content.lower() for keyword in ["reformulated", "rewritten", "rephrased", "clarified"]):
                rewrite_count += 1
    
    # If we've rewritten the question too many times, just generate an answer
    if rewrite_count >= 3:  # Increased threshold for safety
        logger.warning(f"Question rewritten {rewrite_count} times, forcing answer generation")
        return "generate_answer"

    prompt = config['prompts']['GRADE_PROMPT'].format(question=question, context=context)

    print("grade prompt : \n",prompt)
    
    # Log the prompt being sent to the grader
    logger.info("=== GRADE_DOCUMENTS PROMPT ===")
    logger.info(prompt)
    logger.info("=== END GRADE_DOCUMENTS PROMPT ===")
    
    response = (
        grader_model
        .with_structured_output(GradeDocuments).invoke(
            [{"role": "user", "content": prompt}]
        )
    )
    score = response.binary_score
    
    # logger.info(f"GRADING RESULT: {score}")
    
    if score == "yes":
        return "generate_answer"
    else:
        return "rewrite_question"

def rewrite_question(state: MessagesState):
    """Rewrite the original user question."""
    messages = state["messages"]
    question = messages[0].content
    prompt = config['prompts']['REWRITE_PROMPT'].format(question=question)
    
    #Log the prompt being sent to the rewriter
    logger.info("=== REWRITE_QUESTION PROMPT ===")
    logger.info(prompt)
    logger.info("=== END REWRITE_QUESTION PROMPT ===")
    
    response = response_model.invoke([{"role": "user", "content": prompt}])
    return {"messages": [{"role": "user", "content": response.content}]}


def generate_answer(state: MessagesState):
    """Generate an answer."""
    question = state["messages"][0].content
    context = state["messages"][-1].content
    
    # Preprocess the question to handle "provided document" references
    processed_question = question
    if "provided document" in question.lower() or "the document" in question.lower():
        processed_question = question.replace("provided document", "retrieved document content").replace("the document", "the retrieved document content")
    
    prompt = config['prompts']['GENERATE_PROMPT'].format(question=processed_question, context=context)
    
    # Log the prompt being sent to the generator
    logger.info("=== GENERATE_ANSWER PROMPT ===")
    logger.info(f"Original Question: {question}")
    logger.info(f"Processed Question: {processed_question}")
    logger.info("FULL PROMPT:")
    logger.info(prompt)
    logger.info("=== END GENERATE_ANSWER PROMPT ===")
    
    response = response_model.invoke([{"role": "user", "content": prompt}])
    return {"messages": [response]}
